{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "条件概率分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
      "[(',', 2318), ('the', 2048), ('.', 1549), ('of', 1299), ('and', 1103)]\n",
      "2048\n",
      "                  can could   may might  must  will \n",
      "           news    93    86    66    38    50   389 \n",
      "       religion    82    59    78    12    54    71 \n",
      "        hobbies   268    58   131    22    83   264 \n",
      "science_fiction    16    49     4    12     8    16 \n",
      "        romance    74   193    11    51    45    43 \n",
      "          humor    16    30     8     8     9    13 \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "\n",
    "# (condition, event) pair as parameter for CFD\n",
    "cfd = nltk.ConditionalFreqDist((genre, word) \n",
    "                               for genre in brown.categories() \n",
    "                               for word in brown.words(categories=genre))\n",
    "\n",
    "print(cfd.conditions())\n",
    "print(cfd['reviews'].most_common(5))\n",
    "print(cfd['reviews']['the'])\n",
    "\n",
    "# conditions to be display\n",
    "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
    "# events to be display\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
    "# display a table consist of the above conditions&events\n",
    "cfd.tabulate(conditions=genres, samples=modals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CFD+Bigrams 生成随机文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There went in unto them in the sons , the name of his son . Then Abraham said to thy father\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def get_language_model():\n",
    "    words = nltk.corpus.genesis.words('english-kjv.txt')\n",
    "    # bigrams() takes a list of words and builds a list of consecutive word pairs.\n",
    "    bigrams = nltk.bigrams(words)\n",
    "    # the first item of bigram as condition, the second item of bigram as event\n",
    "    cfd = nltk.ConditionalFreqDist(bigrams)\n",
    "    return cfd\n",
    "\n",
    "def generate_language(cfd, word, num=20, top_k=5):\n",
    "    words = [word]\n",
    "    for i in range(num):\n",
    "        if top_k > 0:\n",
    "            word = random.choice(cfd[word].most_common(top_k))[0]\n",
    "        else:\n",
    "            word = cfd[word].max()\n",
    "        words.append(word)\n",
    "    return words\n",
    "\n",
    "words = generate_language(get_language_model(), 'There')\n",
    "print(' '.join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模拟退化分词法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "63 ['doyou', 'seethekitt', 'y', 'see', 'thedoggy', 'doyou', 'liketh', 'ekitt', 'y', 'li', 'ke', 'thedoggy']\n",
      "63 ['doyou', 'seethekitt', 'y', 'see', 'thedoggy', 'doyou', 'liketh', 'ekitt', 'y', 'li', 'ke', 'thedoggy']\n",
      "61 ['doyou', 'see', 't', 'hekitty', 'see', 'thedoggy', 'doyou', 'liketheki', 't', 'ty', 'li', 'ke', 'thedoggy']\n",
      "60 ['doy', 'ou', 'see', 't', 'heki', 't', 'ty', 'see', 'thedoggy', 'doy', 'ou', 'li', 'ket', 'heki', 't', 'ty', 'like', 'thedoggy']\n",
      "60 ['doy', 'ou', 'see', 't', 'heki', 't', 'ty', 'see', 'thedoggy', 'doy', 'ou', 'li', 'ket', 'heki', 't', 'ty', 'like', 'thedoggy']\n",
      "59 ['doy', 'ou', 'see', 't', 'heki', 't', 'ty', 'see', 'thedoggy', 'do', 'y', 'ou', 'like', 't', 'heki', 't', 'ty', 'like', 'thedoggy']\n",
      "53 ['doy', 'ou', 'see', 't', 'heki', 't', 'ty', 'see', 'thedoggy', 'doy', 'ou', 'like', 't', 'heki', 't', 'ty', 'like', 'thedoggy']\n",
      "52 ['doy', 'ou', 'see', 't', 'hekit', 'ty', 'see', 'thedoggy', 'doy', 'ou', 'like', 't', 'hekit', 'ty', 'like', 'thedoggy']\n",
      "52 ['doy', 'ou', 'see', 't', 'hekit', 'ty', 'see', 'thedoggy', 'doy', 'ou', 'like', 't', 'hekit', 'ty', 'like', 'thedoggy']\n",
      "46 ['doyou', 'see', 'thekit', 'ty', 'see', 'thedoggy', 'doyou', 'like', 'thekit', 'ty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0000100100000001001000000010000100010000000100010000000'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 根据二进制串分词\n",
    "def segment(text, segs):\n",
    "    words = []\n",
    "    last = 0\n",
    "    for i in range(len(segs)):\n",
    "        if segs[i] == '1':\n",
    "            words.append(text[last:i+1])\n",
    "            last = i+1\n",
    "    words.append(text[last:])\n",
    "    return words\n",
    "\n",
    "# 评价分词结果\n",
    "def evaluate(text, segs):\n",
    "    words = segment(text, segs)\n",
    "    text_size = len(words)\n",
    "    lexicon_size = sum(len(word) + 1 for word in set(words))\n",
    "    return text_size + lexicon_size\n",
    "\n",
    "# 查找使得objective function最小化的二进制串（基于非确定性的模拟退火）\n",
    "from random import randint\n",
    "\n",
    "def flip(segs, pos):\n",
    "    return segs[:pos] + str(1-int(segs[pos])) + segs[pos+1:]\n",
    "\n",
    "def flip_n(segs, n):\n",
    "    for i in range(n):\n",
    "        segs = flip(segs, randint(0, len(segs)-1))\n",
    "    return segs\n",
    "\n",
    "def anneal(text, segs, iterations, cooling_rate):\n",
    "    temperature = float(len(segs))\n",
    "    while temperature > 0.5:\n",
    "        best_segs, best = segs, evaluate(text, segs)\n",
    "        for i in range(iterations):\n",
    "            guess = flip_n(segs, round(temperature))\n",
    "            score = evaluate(text, guess)\n",
    "            if score < best:\n",
    "                best, best_segs = score, guess\n",
    "        score, segs = best, best_segs\n",
    "        temperature = temperature / cooling_rate\n",
    "        print(evaluate(text, segs), segment(text, segs))\n",
    "    print()\n",
    "    return segs\n",
    "\n",
    "# 运行结果\n",
    "text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
    "seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
    "anneal(text, seg1, 50000, 1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "名词经常出现在什么词后面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NOUN', 7959),\n",
       " ('DET', 7373),\n",
       " ('ADJ', 4761),\n",
       " ('ADP', 3781),\n",
       " ('.', 2796),\n",
       " ('VERB', 1842),\n",
       " ('CONJ', 938),\n",
       " ('NUM', 894),\n",
       " ('ADV', 186),\n",
       " ('PRT', 94),\n",
       " ('PRON', 19),\n",
       " ('X', 11)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "brown_news_tagged = brown.tagged_words(categories='news', tagset='universal')\n",
    "word_tag_pairs = nltk.bigrams(brown_news_tagged) # 构造经过标注后的单词二元组\n",
    "noun_preceders = [a[1] for (a, b) in word_tag_pairs if b[1] == 'NOUN'] # 统计名词前面单词的词性\n",
    "fdist = nltk.FreqDist(noun_preceders)\n",
    "fdist.most_common() # 根据结果可知名词经常出现在冠词，形容词，动词后面"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各种标注器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基线标注器： 0.13089484257215028\n",
      "正则标注器： 0.20326391789486245\n",
      "查表标注器： 0.5817769556656125\n",
      "一元文法标注器： 0.8121200039868434\n",
      "二元文法标注器： 0.10206319146815508\n",
      "回退的二元文法标注器： 0.8452108043456593\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# 基线标注器\n",
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "tag = 'NN'  # 最常见的词性\n",
    "default_tagger = nltk.DefaultTagger(tag)\n",
    "print('基线标注器：', default_tagger.evaluate(brown_tagged_sents))\n",
    "\n",
    "# 正则标注器\n",
    "patterns = [\n",
    "\t(r'.*ing$', 'VBG'),               # gerunds\n",
    "\t(r'.*ed$', 'VBD'),                # simple past\n",
    "\t(r'.*es$', 'VBZ'),                # 3rd singular present\n",
    "\t(r'.*ould$', 'MD'),               # modals\n",
    "\t(r'.*\\'s$', 'NN$'),               # possessive nouns\n",
    "\t(r'.*s$', 'NNS'),                 # plural nouns\n",
    "\t(r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  # cardinal numbers\n",
    "\t(r'.*', 'NN')                     # nouns (default)\n",
    "]\n",
    "\n",
    "regexp_tagger = nltk.RegexpTagger(patterns)\n",
    "print('正则标注器：', regexp_tagger.evaluate(brown_tagged_sents))\n",
    "\n",
    "\n",
    "# 查表标注器\n",
    "brown_sents = brown.sents(categories='news')\n",
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "\n",
    "# 用于查找前 100 高频词\n",
    "fd = nltk.FreqDist(w for w in brown.words(categories='news'))\n",
    "# 用于查找高频词最可能的词性\n",
    "cfd = nltk.ConditionalFreqDist((w, t) for (w, t) in brown.tagged_words(categories='news'))\n",
    "# 生成词性查询表\n",
    "likely_tags = dict((w, cfd[w].max()) for (w, _) in fd.most_common(100))\n",
    "\n",
    "baseline_tagger = nltk.UnigramTagger(model=likely_tags, backoff=nltk.DefaultTagger('NN'))  # 当词不在表中时，指定回退标注器\n",
    "baseline_tagger.tag(brown_sents[3])\n",
    "print('查表标注器：', baseline_tagger.evaluate(brown_tagged_sents))\n",
    "\n",
    "# 二元文法标注器\n",
    "size = int(len(brown_tagged_sents) * 0.9)\n",
    "train_sents = brown_tagged_sents[:size]\n",
    "test_sents = brown_tagged_sents[size:]\n",
    "\n",
    "unigram_tagger = nltk.UnigramTagger(train_sents)\n",
    "print('一元文法标注器：', unigram_tagger.evaluate(test_sents))\n",
    "\n",
    "bigram_tagger = nltk.BigramTagger(train_sents)\n",
    "print('二元文法标注器：', bigram_tagger.evaluate(test_sents))  # 因为未登录词，导致性能较低\n",
    "\n",
    "t0 = nltk.DefaultTagger('NN')\n",
    "t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "print('回退的二元文法标注器：', t2.evaluate(test_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "性别识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neo male\n",
      "Trinity female\n",
      "accuary 0.756\n",
      "Most Informative Features\n",
      "             last_letter = 'a'            female : male   =     35.4 : 1.0\n",
      "             last_letter = 'k'              male : female =     30.0 : 1.0\n",
      "             last_letter = 'f'              male : female =     16.7 : 1.0\n",
      "             last_letter = 'p'              male : female =     12.6 : 1.0\n",
      "             last_letter = 'v'              male : female =     11.3 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import names\n",
    "\n",
    "# 定义feature extactor\n",
    "def gender_features(name):\n",
    "\treturn {'last_letter': name[-1:]} # return feature set\n",
    "\n",
    "# 构造(input, label)数据\n",
    "labeled_names = [(name, 'male') for name in names.words('male.txt')] + [(name, 'female') for name in names.words('female.txt')]\n",
    "random.shuffle(labeled_names)\n",
    "\n",
    "# 构造(feature-set, label)，并将数据划分为training和testing集\n",
    "featuresets = [(gender_features(name), label) for (name, label) in labeled_names]\n",
    "train_set, test_set = featuresets[500:], featuresets[:500]\n",
    "\n",
    "# 构造训练集和测试集时节省内存的方法\n",
    "from nltk.classify import apply_features\n",
    "train_set = apply_features(gender_features, labeled_names[500:])\n",
    "test_set = apply_features(gender_features, labeled_names[:500])\n",
    "\n",
    "# 训练分类器\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# 对名字分类，注这里使用了训练时的特征提取器\n",
    "name = 'Neo'\n",
    "print(name, classifier.classify(gender_features(name)))\n",
    "name = 'Trinity'\n",
    "print(name, classifier.classify(gender_features(name)))\n",
    "\n",
    "# 使用测试数据对分类器评分\n",
    "print('accuary', nltk.classify.accuracy(classifier, test_set))\n",
    "\n",
    "# 查看前5个特征在分类时的贡献\n",
    "print(classifier.show_most_informative_features(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文档情感分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'feature_probdist'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-22f007d44b52>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# build classifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNaiveBayesClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow_most_informative_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'feature_probdist'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# feature extractor\n",
    "# 以文档中是否含有文本集中高频的2000单词作为特征集\n",
    "word_features = list(nltk.FreqDist(w.lower() for w in movie_reviews.words()))[:2000] # 2000 most frequent words as feature-words\n",
    "\n",
    "def document_features(document):\n",
    "\tdocument = set(document) # set faster than list to check if contains a element\n",
    "\tfeatures = {}\n",
    "\tfor w in word_features:\n",
    "\t\tfeatures['contains({})'.format(w)] = (w in document)\n",
    "\treturn features\n",
    "\n",
    "# build (document-words, label)\n",
    "documents = [(list(movie_reviews.words(fileid)), category) for category in movie_reviews.categories() for fileid in movie_reviews.fileids(category)] # 'pos' and 'neg' categories\n",
    "random.shuffle(documents)\n",
    "\n",
    "# build (featureset, label), and split into trainingt/test set\n",
    "featuresets = [(document_features(document), label) for (document, label) in documents]\n",
    "train_set, test_set = featuresets[:100], featuresets[100:]\n",
    "\n",
    "# build classifier\n",
    "classifier = nltk.NaiveBayesClassifier(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)\n",
    "classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词性标注器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-465a746f9096>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;31m# build DecisionTreeClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cats'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda3\\envs\\nlp\\lib\\site-packages\\nltk\\classify\\decisiontree.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(labeled_featuresets, entropy_cutoff, depth_cutoff, support_cutoff, binary, feature_values, verbose)\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[0mfeature_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlabeled_featuresets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeatureset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m                 \u001b[0mfeature_names\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# build feature extractor\n",
    "fd = nltk.FreqDist()\n",
    "for word in brown.words():\n",
    "\tword = word.lower()\n",
    "\tfd[word[-1:]] += 1\n",
    "\tfd[word[-2:]] += 1\n",
    "\tfd[word[-3:]] += 1\n",
    "common_suffixes = [suffix for (suffix, count) in fd.most_common(100)]\n",
    "\n",
    "def pos_features(word):\n",
    "\tfeatures = {}\n",
    "\tfor suffix in common_suffixes:\n",
    "\t\tfeatures['endswith({})'.format(suffix)] = word.lower().endswith(suffix)\n",
    "\n",
    "# build (featureset, label) and split into train/test set\n",
    "featuresets = [(pos_features(n), g) for (n,g) in brown.tagged_words(categories='news')]\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "\n",
    "# build DecisionTreeClassifier\n",
    "classifier = nltk.DecisionTreeClassifier.train(train_set)\n",
    "classifier.classify(pos_features('cats'))\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "信息提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP the/DT cat/NN))\n"
     ]
    }
   ],
   "source": [
    "import nltk, re, pprint\n",
    "from IPython.display import display\n",
    "\n",
    "def preprocess(document):\n",
    "    # sentence segment\n",
    "    sentences = nltk.sent_tokenize(document)\n",
    "    # tokenize\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    # POS tag\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "\n",
    "\n",
    "def simple_chunk_grammar(sentence):\n",
    "    grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    result = cp.parse(sentence)\n",
    "    print(result)\n",
    "    result.draw()\n",
    "    \n",
    "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"),\n",
    "    (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
    "\n",
    "simple_chunk_grammar(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, pprint\n",
    "\n",
    "grammar = r\"\"\"\n",
    "  NP: {<DT|JJ|NN.*>+}          # Chunk sequences of DT, JJ, NN\n",
    "  PP: {<IN><NP>}               # Chunk prepositions followed by NP\n",
    "  VP: {<VB.*><NP|PP|CLAUSE>+$} # Chunk verbs and their arguments\n",
    "  CLAUSE: {<NP><VP>}           # Chunk NP, VP\n",
    "  \"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "sentence = [(\"Mary\", \"NN\"), (\"saw\", \"VBD\"), (\"the\", \"DT\"), (\"cat\", \"NN\"),\n",
    "    (\"sit\", \"VB\"), (\"on\", \"IN\"), (\"the\", \"DT\"), (\"mat\", \"NN\")]\n",
    "    \n",
    "tree = cp.parse(sentence)\n",
    "tree.draw()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
