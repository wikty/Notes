{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "条件概率分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n",
      "[(',', 2318), ('the', 2048), ('.', 1549), ('of', 1299), ('and', 1103)]\n",
      "2048\n",
      "                  can could   may might  must  will \n",
      "           news    93    86    66    38    50   389 \n",
      "       religion    82    59    78    12    54    71 \n",
      "        hobbies   268    58   131    22    83   264 \n",
      "science_fiction    16    49     4    12     8    16 \n",
      "        romance    74   193    11    51    45    43 \n",
      "          humor    16    30     8     8     9    13 \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "\n",
    "# (condition, event) pair as parameter for CFD\n",
    "cfd = nltk.ConditionalFreqDist((genre, word) \n",
    "                               for genre in brown.categories() \n",
    "                               for word in brown.words(categories=genre))\n",
    "\n",
    "print(cfd.conditions())\n",
    "print(cfd['reviews'].most_common(5))\n",
    "print(cfd['reviews']['the'])\n",
    "\n",
    "# conditions to be display\n",
    "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
    "# events to be display\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
    "# display a table consist of the above conditions&events\n",
    "cfd.tabulate(conditions=genres, samples=modals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CFD+Bigrams 生成随机文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There went in unto them in the sons , the name of his son . Then Abraham said to thy father\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def get_language_model():\n",
    "    words = nltk.corpus.genesis.words('english-kjv.txt')\n",
    "    # bigrams() takes a list of words and builds a list of consecutive word pairs.\n",
    "    bigrams = nltk.bigrams(words)\n",
    "    # the first item of bigram as condition, the second item of bigram as event\n",
    "    cfd = nltk.ConditionalFreqDist(bigrams)\n",
    "    return cfd\n",
    "\n",
    "def generate_language(cfd, word, num=20, top_k=5):\n",
    "    words = [word]\n",
    "    for i in range(num):\n",
    "        if top_k > 0:\n",
    "            word = random.choice(cfd[word].most_common(top_k))[0]\n",
    "        else:\n",
    "            word = cfd[word].max()\n",
    "        words.append(word)\n",
    "    return words\n",
    "\n",
    "words = generate_language(get_language_model(), 'There')\n",
    "print(' '.join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模拟退化分词法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "63 ['doyou', 'seethekitt', 'y', 'see', 'thedoggy', 'doyou', 'liketh', 'ekitt', 'y', 'li', 'ke', 'thedoggy']\n",
      "63 ['doyou', 'seethekitt', 'y', 'see', 'thedoggy', 'doyou', 'liketh', 'ekitt', 'y', 'li', 'ke', 'thedoggy']\n",
      "61 ['doyou', 'see', 't', 'hekitty', 'see', 'thedoggy', 'doyou', 'liketheki', 't', 'ty', 'li', 'ke', 'thedoggy']\n",
      "60 ['doy', 'ou', 'see', 't', 'heki', 't', 'ty', 'see', 'thedoggy', 'doy', 'ou', 'li', 'ket', 'heki', 't', 'ty', 'like', 'thedoggy']\n",
      "60 ['doy', 'ou', 'see', 't', 'heki', 't', 'ty', 'see', 'thedoggy', 'doy', 'ou', 'li', 'ket', 'heki', 't', 'ty', 'like', 'thedoggy']\n",
      "59 ['doy', 'ou', 'see', 't', 'heki', 't', 'ty', 'see', 'thedoggy', 'do', 'y', 'ou', 'like', 't', 'heki', 't', 'ty', 'like', 'thedoggy']\n",
      "53 ['doy', 'ou', 'see', 't', 'heki', 't', 'ty', 'see', 'thedoggy', 'doy', 'ou', 'like', 't', 'heki', 't', 'ty', 'like', 'thedoggy']\n",
      "52 ['doy', 'ou', 'see', 't', 'hekit', 'ty', 'see', 'thedoggy', 'doy', 'ou', 'like', 't', 'hekit', 'ty', 'like', 'thedoggy']\n",
      "52 ['doy', 'ou', 'see', 't', 'hekit', 'ty', 'see', 'thedoggy', 'doy', 'ou', 'like', 't', 'hekit', 'ty', 'like', 'thedoggy']\n",
      "46 ['doyou', 'see', 'thekit', 'ty', 'see', 'thedoggy', 'doyou', 'like', 'thekit', 'ty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0000100100000001001000000010000100010000000100010000000'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 根据二进制串分词\n",
    "def segment(text, segs):\n",
    "    words = []\n",
    "    last = 0\n",
    "    for i in range(len(segs)):\n",
    "        if segs[i] == '1':\n",
    "            words.append(text[last:i+1])\n",
    "            last = i+1\n",
    "    words.append(text[last:])\n",
    "    return words\n",
    "\n",
    "# 评价分词结果\n",
    "def evaluate(text, segs):\n",
    "    words = segment(text, segs)\n",
    "    text_size = len(words)\n",
    "    lexicon_size = sum(len(word) + 1 for word in set(words))\n",
    "    return text_size + lexicon_size\n",
    "\n",
    "# 查找使得objective function最小化的二进制串（基于非确定性的模拟退火）\n",
    "from random import randint\n",
    "\n",
    "def flip(segs, pos):\n",
    "    return segs[:pos] + str(1-int(segs[pos])) + segs[pos+1:]\n",
    "\n",
    "def flip_n(segs, n):\n",
    "    for i in range(n):\n",
    "        segs = flip(segs, randint(0, len(segs)-1))\n",
    "    return segs\n",
    "\n",
    "def anneal(text, segs, iterations, cooling_rate):\n",
    "    temperature = float(len(segs))\n",
    "    while temperature > 0.5:\n",
    "        best_segs, best = segs, evaluate(text, segs)\n",
    "        for i in range(iterations):\n",
    "            guess = flip_n(segs, round(temperature))\n",
    "            score = evaluate(text, guess)\n",
    "            if score < best:\n",
    "                best, best_segs = score, guess\n",
    "        score, segs = best, best_segs\n",
    "        temperature = temperature / cooling_rate\n",
    "        print(evaluate(text, segs), segment(text, segs))\n",
    "    print()\n",
    "    return segs\n",
    "\n",
    "# 运行结果\n",
    "text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
    "seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
    "anneal(text, seg1, 50000, 1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "名词经常出现在什么词后面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NOUN', 7959),\n",
       " ('DET', 7373),\n",
       " ('ADJ', 4761),\n",
       " ('ADP', 3781),\n",
       " ('.', 2796),\n",
       " ('VERB', 1842),\n",
       " ('CONJ', 938),\n",
       " ('NUM', 894),\n",
       " ('ADV', 186),\n",
       " ('PRT', 94),\n",
       " ('PRON', 19),\n",
       " ('X', 11)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "brown_news_tagged = brown.tagged_words(categories='news', tagset='universal')\n",
    "word_tag_pairs = nltk.bigrams(brown_news_tagged) # 构造经过标注后的单词二元组\n",
    "noun_preceders = [a[1] for (a, b) in word_tag_pairs if b[1] == 'NOUN'] # 统计名词前面单词的词性\n",
    "fdist = nltk.FreqDist(noun_preceders)\n",
    "fdist.most_common() # 根据结果可知名词经常出现在冠词，形容词，动词后面"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各种标注器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "基线标注器： 0.13089484257215028\n",
      "正则标注器： 0.20326391789486245\n",
      "查表标注器： 0.5817769556656125\n",
      "一元文法标注器： 0.8121200039868434\n",
      "二元文法标注器： 0.10206319146815508\n",
      "回退的二元文法标注器： 0.8452108043456593\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# 基线标注器\n",
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "tag = 'NN'  # 最常见的词性\n",
    "default_tagger = nltk.DefaultTagger(tag)\n",
    "print('基线标注器：', default_tagger.evaluate(brown_tagged_sents))\n",
    "\n",
    "# 正则标注器\n",
    "patterns = [\n",
    "\t(r'.*ing$', 'VBG'),               # gerunds\n",
    "\t(r'.*ed$', 'VBD'),                # simple past\n",
    "\t(r'.*es$', 'VBZ'),                # 3rd singular present\n",
    "\t(r'.*ould$', 'MD'),               # modals\n",
    "\t(r'.*\\'s$', 'NN$'),               # possessive nouns\n",
    "\t(r'.*s$', 'NNS'),                 # plural nouns\n",
    "\t(r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  # cardinal numbers\n",
    "\t(r'.*', 'NN')                     # nouns (default)\n",
    "]\n",
    "\n",
    "regexp_tagger = nltk.RegexpTagger(patterns)\n",
    "print('正则标注器：', regexp_tagger.evaluate(brown_tagged_sents))\n",
    "\n",
    "\n",
    "# 查表标注器\n",
    "brown_sents = brown.sents(categories='news')\n",
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "\n",
    "# 用于查找前 100 高频词\n",
    "fd = nltk.FreqDist(w for w in brown.words(categories='news'))\n",
    "# 用于查找高频词最可能的词性\n",
    "cfd = nltk.ConditionalFreqDist((w, t) for (w, t) in brown.tagged_words(categories='news'))\n",
    "# 生成词性查询表\n",
    "likely_tags = dict((w, cfd[w].max()) for (w, _) in fd.most_common(100))\n",
    "\n",
    "baseline_tagger = nltk.UnigramTagger(model=likely_tags, backoff=nltk.DefaultTagger('NN'))  # 当词不在表中时，指定回退标注器\n",
    "baseline_tagger.tag(brown_sents[3])\n",
    "print('查表标注器：', baseline_tagger.evaluate(brown_tagged_sents))\n",
    "\n",
    "# 二元文法标注器\n",
    "size = int(len(brown_tagged_sents) * 0.9)\n",
    "train_sents = brown_tagged_sents[:size]\n",
    "test_sents = brown_tagged_sents[size:]\n",
    "\n",
    "unigram_tagger = nltk.UnigramTagger(train_sents)\n",
    "print('一元文法标注器：', unigram_tagger.evaluate(test_sents))\n",
    "\n",
    "bigram_tagger = nltk.BigramTagger(train_sents)\n",
    "print('二元文法标注器：', bigram_tagger.evaluate(test_sents))  # 因为未登录词，导致性能较低\n",
    "\n",
    "t0 = nltk.DefaultTagger('NN')\n",
    "t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "print('回退的二元文法标注器：', t2.evaluate(test_sents))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
